{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Обучите CNN (самописная) на CIFAR-100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Определите размер карты признаков на выходе, если на вход слоя свертки пришел тензор 1х128х100х100, а сверточный слой имеет фильтр размера 3, шаг = 1 и использует заполнение 0  \n",
    "1 128 98 98  \n",
    "\n",
    "E. Определите размер карты признаков на выходе, если на вход слоя пуллинга пришел тензор 1х128х100х100, а сверточный слой имеет фильтр размера 2, шаг = 2 и использует заполнение 0  \n",
    "1 128 50 50 (шаг 2 уменьшает в 2 раза размер)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vg2xLldIm5Dk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KS8p8olnSjgx",
    "outputId": "26cff7dd-e10f-4254-acd3-09c1987ce212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\pc\\anaconda3\\envs\\py310torch\\lib\\site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gUPTXDVWU_O7",
    "outputId": "ccebd86a-95cb-487b-b8a9-ba5cf854bb6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3FDqafKSkB2"
   },
   "source": [
    "## Прогрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kT8_tOUNm6yZ",
    "outputId": "119b4491-301b-4ae1-8e58-b7eab9554ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR100(root='data/',\n",
    "                                             train=True,\n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCqAGAq2q1N3",
    "outputId": "cfacd067-a17e-44e3-bd47-5b2f75e4ae72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HAp2tZfRm61m"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=500,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=500,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzOHgLfMyhbd"
   },
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7hGnf55vWcP",
    "outputId": "64d4f11f-66aa-4fb9-c1d5-df9a1682e748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Dropout-1            [-1, 3, 32, 32]               0\n",
      "       BatchNorm2d-2            [-1, 3, 32, 32]               6\n",
      "            Conv2d-3           [-1, 30, 32, 32]             840\n",
      "       BatchNorm2d-4           [-1, 30, 32, 32]              60\n",
      "            Conv2d-5           [-1, 64, 32, 32]          17,344\n",
      "           Dropout-6           [-1, 64, 32, 32]               0\n",
      "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
      "            Conv2d-8          [-1, 128, 32, 32]          73,856\n",
      "           Dropout-9          [-1, 128, 32, 32]               0\n",
      "      BatchNorm2d-10          [-1, 128, 16, 16]             256\n",
      "           Conv2d-11          [-1, 128, 14, 14]         147,584\n",
      "          Dropout-12          [-1, 128, 14, 14]               0\n",
      "      BatchNorm2d-13          [-1, 128, 14, 14]             256\n",
      "           Conv2d-14          [-1, 256, 12, 12]         295,168\n",
      "          Dropout-15          [-1, 256, 12, 12]               0\n",
      "      BatchNorm2d-16            [-1, 256, 6, 6]             512\n",
      "           Conv2d-17            [-1, 256, 4, 4]         590,080\n",
      "      BatchNorm2d-18            [-1, 256, 4, 4]             512\n",
      "           Conv2d-19            [-1, 512, 2, 2]       1,180,160\n",
      "      BatchNorm2d-20            [-1, 512, 2, 2]           1,024\n",
      "           Linear-21                 [-1, 1024]       2,098,176\n",
      "          Dropout-22                 [-1, 1024]               0\n",
      "           Linear-23                  [-1, 512]         524,800\n",
      "          Dropout-24                  [-1, 512]               0\n",
      "           Linear-25                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 4,982,062\n",
      "Trainable params: 4,982,062\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 5.59\n",
      "Params size (MB): 19.01\n",
      "Estimated Total Size (MB): 24.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Net_CCN_5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net_CCN_5, self).__init__()\n",
    "\n",
    "        self.dp1 = nn.Dropout(0.2)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(3)\n",
    "        self.conv1 = torch.nn.Conv2d(3, 30, 3)#32\n",
    "\n",
    "        self.bn2 = torch.nn.BatchNorm2d(30)\n",
    "        self.conv2 = torch.nn.Conv2d(30, 64, 3) #32\n",
    "        self.dp2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.bn3= torch.nn.BatchNorm2d(64)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 128, 3) #32 вход\n",
    "        self.dp3 = nn.Dropout(0.3)\n",
    "        #polling запулингировали 16\n",
    "\n",
    "        self.bn4 = torch.nn.BatchNorm2d(128)\n",
    "        self.conv4 = torch.nn.Conv2d(128, 128, 3) #16 в 14\n",
    "        self.dp4 = nn.Dropout(0.2)\n",
    "\n",
    "        self.bn5= torch.nn.BatchNorm2d(128)\n",
    "        self.conv5 = torch.nn.Conv2d(128, 256, 3) #14 в 12\n",
    "        self.dp5 = nn.Dropout(0.2)\n",
    "\n",
    "        #polling #из 12 в 6\n",
    "\n",
    "        self.bn6 = torch.nn.BatchNorm2d(256)\n",
    "        self.conv6 = torch.nn.Conv2d(256, 256, 3) #из 6 в 4\n",
    "\n",
    "        self.bn7 = torch.nn.BatchNorm2d(256)\n",
    "        self.conv7 = torch.nn.Conv2d(256, 512, 3) # 2\n",
    "        #self.fl = torch.nn.Flatten(2)\n",
    "\n",
    "        self.bn8 = torch.nn.BatchNorm2d(512)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(2048, 1024) #256 на 3 на 3\n",
    "        self.dp_fc1 = nn.Dropout(0.3)\n",
    "        self.fc2 = torch.nn.Linear(1024, 512)\n",
    "        self.dp_fc2 = nn.Dropout(0.3)\n",
    "        self.out = torch.nn.Linear(512, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.dp1(x)\n",
    "        x = self.bn1(x) # 32 на 32\n",
    "        x = torch.nn.functional.pad(x, (1, 1, 1, 1), mode='reflect')\n",
    "        x = self.conv1(x) # 32 на 32 падинг 1\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.bn2(x)\n",
    "        x = torch.nn.functional.pad(x, (1, 1, 1, 1), mode='reflect')\n",
    "        x = self.conv2(x) #30 на 30\n",
    "        x = self.dp2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.bn3(x)\n",
    "        x = torch.nn.functional.pad(x, (1, 1, 1, 1), mode='reflect')\n",
    "        x = self.conv3(x) #28 на 28\n",
    "        x = self.dp3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.avg_pool2d(x,2,stride=2) #14 на 14\n",
    "\n",
    "        x = self.bn4(x) #14 на 14\n",
    "        x = self.conv4(x) #12 на 12\n",
    "        x = self.dp4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.bn5(x)\n",
    "        x = self.conv5(x) #\n",
    "        x = self.dp5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.avg_pool2d(x,2,stride=2)\n",
    "\n",
    "        x = self.bn6(x)\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.bn7(x)\n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.fl(x)\n",
    "        x = self.bn8(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dp_fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.dp_fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self.out(x)\n",
    "\n",
    "net = Net_CCN_5().to(device)\n",
    "#print(net)\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(net, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4c6OQz_SRz2D"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=0.007)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WG-yph_rRz4n",
    "outputId": "6b8264ae-80a2-41bb-c2fd-4e6dcf9ff424",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]. Step [1/100]. Loss: 0.0092. Acc: 0.8000. Test acc: 1.0000\n",
      "Epoch [2/100]. Step [1/100]. Loss: 0.0092. Acc: 2.6000. Test acc: 2.1000\n",
      "Epoch [3/100]. Step [1/100]. Loss: 0.0090. Acc: 2.0000. Test acc: 2.6900\n",
      "Epoch [4/100]. Step [1/100]. Loss: 0.0089. Acc: 2.4000. Test acc: 3.2900\n",
      "Epoch [5/100]. Step [1/100]. Loss: 0.0089. Acc: 3.0000. Test acc: 3.2900\n",
      "Epoch [6/100]. Step [1/100]. Loss: 0.0086. Acc: 4.2000. Test acc: 4.4100\n",
      "Epoch [7/100]. Step [1/100]. Loss: 0.0084. Acc: 5.2000. Test acc: 3.5600\n",
      "Epoch [8/100]. Step [1/100]. Loss: 0.0086. Acc: 4.0000. Test acc: 4.1900\n",
      "Epoch [9/100]. Step [1/100]. Loss: 0.0089. Acc: 4.6000. Test acc: 4.1300\n",
      "Epoch [10/100]. Step [1/100]. Loss: 0.0084. Acc: 7.0000. Test acc: 5.2100\n",
      "Epoch [11/100]. Step [1/100]. Loss: 0.0084. Acc: 3.4000. Test acc: 5.2200\n",
      "Epoch [12/100]. Step [1/100]. Loss: 0.0085. Acc: 6.8000. Test acc: 4.8800\n",
      "Epoch [13/100]. Step [1/100]. Loss: 0.0085. Acc: 4.6000. Test acc: 5.3600\n",
      "Epoch [14/100]. Step [1/100]. Loss: 0.0081. Acc: 8.4000. Test acc: 5.9300\n",
      "Epoch [15/100]. Step [1/100]. Loss: 0.0081. Acc: 5.4000. Test acc: 5.7000\n",
      "Epoch [16/100]. Step [1/100]. Loss: 0.0078. Acc: 8.4000. Test acc: 6.0800\n",
      "Epoch [17/100]. Step [1/100]. Loss: 0.0080. Acc: 8.2000. Test acc: 5.3900\n",
      "Epoch [18/100]. Step [1/100]. Loss: 0.0079. Acc: 8.4000. Test acc: 5.1000\n",
      "Epoch [19/100]. Step [1/100]. Loss: 0.0109. Acc: 6.8000. Test acc: 4.0300\n",
      "Epoch [20/100]. Step [1/100]. Loss: 0.0079. Acc: 9.2000. Test acc: 6.9500\n",
      "Epoch [21/100]. Step [1/100]. Loss: 0.0076. Acc: 12.2000. Test acc: 10.6300\n",
      "Epoch [22/100]. Step [1/100]. Loss: 0.0075. Acc: 13.2000. Test acc: 8.3000\n",
      "Epoch [23/100]. Step [1/100]. Loss: 0.0074. Acc: 13.8000. Test acc: 9.9300\n",
      "Epoch [24/100]. Step [1/100]. Loss: 0.0074. Acc: 14.2000. Test acc: 13.2000\n",
      "Epoch [25/100]. Step [1/100]. Loss: 0.0068. Acc: 18.6000. Test acc: 12.7800\n",
      "Epoch [26/100]. Step [1/100]. Loss: 0.0073. Acc: 13.0000. Test acc: 10.7800\n",
      "Epoch [27/100]. Step [1/100]. Loss: 0.0068. Acc: 18.0000. Test acc: 12.0800\n",
      "Epoch [28/100]. Step [1/100]. Loss: 0.0067. Acc: 19.0000. Test acc: 13.2600\n",
      "Epoch [29/100]. Step [1/100]. Loss: 0.0091. Acc: 2.2000. Test acc: 1.6600\n",
      "Epoch [30/100]. Step [1/100]. Loss: 0.0070. Acc: 16.6000. Test acc: 12.8000\n",
      "Epoch [31/100]. Step [1/100]. Loss: 0.0067. Acc: 19.6000. Test acc: 13.7900\n",
      "Epoch [32/100]. Step [1/100]. Loss: 0.0067. Acc: 18.4000. Test acc: 11.5600\n",
      "Epoch [33/100]. Step [1/100]. Loss: 0.0068. Acc: 16.6000. Test acc: 13.3800\n",
      "Epoch [34/100]. Step [1/100]. Loss: 0.0064. Acc: 21.0000. Test acc: 13.7400\n",
      "Epoch [35/100]. Step [1/100]. Loss: 0.0063. Acc: 22.8000. Test acc: 16.0900\n",
      "Epoch [36/100]. Step [1/100]. Loss: 0.0065. Acc: 20.2000. Test acc: 16.5900\n",
      "Epoch [37/100]. Step [1/100]. Loss: 0.0063. Acc: 24.2000. Test acc: 15.7800\n",
      "Epoch [38/100]. Step [1/100]. Loss: 0.0063. Acc: 20.6000. Test acc: 15.5900\n",
      "Epoch [39/100]. Step [1/100]. Loss: 0.0063. Acc: 23.4000. Test acc: 15.9100\n",
      "Epoch [40/100]. Step [1/100]. Loss: 0.0062. Acc: 24.2000. Test acc: 18.2400\n",
      "Epoch [41/100]. Step [1/100]. Loss: 0.0061. Acc: 27.4000. Test acc: 19.1800\n",
      "Epoch [42/100]. Step [1/100]. Loss: 0.0056. Acc: 29.6000. Test acc: 18.6800\n",
      "Epoch [43/100]. Step [1/100]. Loss: 0.0058. Acc: 28.0000. Test acc: 17.0300\n",
      "Epoch [44/100]. Step [1/100]. Loss: 0.0061. Acc: 23.2000. Test acc: 17.7500\n",
      "Epoch [45/100]. Step [1/100]. Loss: 0.0057. Acc: 28.2000. Test acc: 20.9400\n",
      "Epoch [46/100]. Step [1/100]. Loss: 0.0057. Acc: 30.2000. Test acc: 22.4500\n",
      "Epoch [47/100]. Step [1/100]. Loss: 0.0054. Acc: 33.0000. Test acc: 24.1100\n",
      "Epoch [48/100]. Step [1/100]. Loss: 0.0059. Acc: 29.6000. Test acc: 20.8300\n",
      "Epoch [49/100]. Step [1/100]. Loss: 0.0060. Acc: 26.6000. Test acc: 20.9400\n",
      "Epoch [50/100]. Step [1/100]. Loss: 0.0053. Acc: 32.0000. Test acc: 23.1200\n",
      "Epoch [51/100]. Step [1/100]. Loss: 0.0057. Acc: 30.2000. Test acc: 21.2200\n",
      "Epoch [52/100]. Step [1/100]. Loss: 0.0054. Acc: 31.6000. Test acc: 21.6700\n",
      "Epoch [53/100]. Step [1/100]. Loss: 0.0052. Acc: 33.4000. Test acc: 24.6600\n",
      "Epoch [54/100]. Step [1/100]. Loss: 0.0053. Acc: 34.0000. Test acc: 21.5600\n",
      "Epoch [55/100]. Step [1/100]. Loss: 0.0050. Acc: 34.0000. Test acc: 27.2200\n",
      "Epoch [56/100]. Step [1/100]. Loss: 0.0053. Acc: 33.0000. Test acc: 27.6300\n",
      "Epoch [57/100]. Step [1/100]. Loss: 0.0050. Acc: 36.0000. Test acc: 23.5700\n",
      "Epoch [58/100]. Step [1/100]. Loss: 0.0057. Acc: 29.6000. Test acc: 21.0000\n",
      "Epoch [59/100]. Step [1/100]. Loss: 0.0051. Acc: 34.4000. Test acc: 26.4900\n",
      "Epoch [60/100]. Step [1/100]. Loss: 0.0048. Acc: 33.0000. Test acc: 29.7000\n",
      "Epoch [61/100]. Step [1/100]. Loss: 0.0048. Acc: 37.6000. Test acc: 28.1600\n",
      "Epoch [62/100]. Step [1/100]. Loss: 0.0052. Acc: 34.2000. Test acc: 30.6500\n",
      "Epoch [63/100]. Step [1/100]. Loss: 0.0049. Acc: 37.0000. Test acc: 31.1700\n",
      "Epoch [64/100]. Step [1/100]. Loss: 0.0048. Acc: 35.2000. Test acc: 28.9400\n",
      "Epoch [65/100]. Step [1/100]. Loss: 0.0049. Acc: 34.8000. Test acc: 29.3600\n",
      "Epoch [66/100]. Step [1/100]. Loss: 0.0050. Acc: 35.4000. Test acc: 29.8200\n",
      "Epoch [67/100]. Step [1/100]. Loss: 0.0049. Acc: 35.8000. Test acc: 28.4900\n",
      "Epoch [68/100]. Step [1/100]. Loss: 0.0044. Acc: 40.6000. Test acc: 27.4900\n",
      "Epoch [69/100]. Step [1/100]. Loss: 0.0047. Acc: 37.6000. Test acc: 27.9900\n",
      "Epoch [70/100]. Step [1/100]. Loss: 0.0044. Acc: 42.4000. Test acc: 29.6500\n",
      "Epoch [71/100]. Step [1/100]. Loss: 0.0045. Acc: 38.6000. Test acc: 29.5100\n",
      "Epoch [72/100]. Step [1/100]. Loss: 0.0042. Acc: 43.4000. Test acc: 29.3700\n",
      "Epoch [73/100]. Step [1/100]. Loss: 0.0044. Acc: 41.2000. Test acc: 32.3900\n",
      "Epoch [74/100]. Step [1/100]. Loss: 0.0045. Acc: 40.4000. Test acc: 31.9800\n",
      "Epoch [75/100]. Step [1/100]. Loss: 0.0045. Acc: 41.8000. Test acc: 30.4600\n",
      "Epoch [76/100]. Step [1/100]. Loss: 0.0045. Acc: 40.6000. Test acc: 30.5000\n",
      "Epoch [77/100]. Step [1/100]. Loss: 0.0044. Acc: 43.8000. Test acc: 32.6600\n",
      "Epoch [78/100]. Step [1/100]. Loss: 0.0043. Acc: 43.0000. Test acc: 32.8700\n",
      "Epoch [79/100]. Step [1/100]. Loss: 0.0045. Acc: 41.2000. Test acc: 27.9900\n",
      "Epoch [80/100]. Step [1/100]. Loss: 0.0037. Acc: 47.6000. Test acc: 33.5500\n",
      "Epoch [81/100]. Step [1/100]. Loss: 0.0041. Acc: 44.8000. Test acc: 33.6500\n",
      "Epoch [82/100]. Step [1/100]. Loss: 0.0040. Acc: 46.8000. Test acc: 31.9700\n",
      "Epoch [83/100]. Step [1/100]. Loss: 0.0040. Acc: 43.2000. Test acc: 30.1700\n",
      "Epoch [84/100]. Step [1/100]. Loss: 0.0041. Acc: 45.2000. Test acc: 32.2500\n",
      "Epoch [85/100]. Step [1/100]. Loss: 0.0078. Acc: 20.2000. Test acc: 16.3100\n",
      "Epoch [86/100]. Step [1/100]. Loss: 0.0040. Acc: 46.8000. Test acc: 29.3300\n",
      "Epoch [87/100]. Step [1/100]. Loss: 0.0041. Acc: 46.2000. Test acc: 30.5200\n",
      "Epoch [88/100]. Step [1/100]. Loss: 0.0040. Acc: 49.0000. Test acc: 33.6400\n",
      "Epoch [89/100]. Step [1/100]. Loss: 0.0039. Acc: 45.6000. Test acc: 33.7100\n",
      "Epoch [90/100]. Step [1/100]. Loss: 0.0041. Acc: 48.4000. Test acc: 34.2200\n",
      "Epoch [91/100]. Step [1/100]. Loss: 0.0034. Acc: 53.6000. Test acc: 33.3900\n",
      "Epoch [92/100]. Step [1/100]. Loss: 0.0039. Acc: 47.2000. Test acc: 33.7800\n",
      "Epoch [93/100]. Step [1/100]. Loss: 0.0039. Acc: 49.2000. Test acc: 35.2400\n",
      "Epoch [94/100]. Step [1/100]. Loss: 0.0037. Acc: 50.0000. Test acc: 35.1300\n",
      "Epoch [95/100]. Step [1/100]. Loss: 0.0034. Acc: 52.6000. Test acc: 35.7700\n",
      "Epoch [96/100]. Step [1/100]. Loss: 0.0037. Acc: 52.6000. Test acc: 34.3400\n",
      "Epoch [97/100]. Step [1/100]. Loss: 0.0035. Acc: 54.8000. Test acc: 35.9300\n",
      "Epoch [98/100]. Step [1/100]. Loss: 0.0037. Acc: 52.0000. Test acc: 31.8300\n",
      "Epoch [99/100]. Step [1/100]. Loss: 0.0030. Acc: 59.2000. Test acc: 36.5100\n",
      "Epoch [100/100]. Step [1/100]. Loss: 0.0036. Acc: 52.2000. Test acc: 33.3700\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)  #to(device) .to(device)\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "        running_right += (labels == torch.max(outputs, 1)[1]).sum()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            net.eval()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {running_loss / running_items:.4f}. ' \\\n",
    "                  f'Acc: {(running_right / running_items)*100:.4f}', end='. ')\n",
    "            running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "\n",
    "            test_running_right, test_running_total = 0.0, 0.0\n",
    "            for i, data in enumerate(test_loader):\n",
    "\n",
    "                test_outputs = net(data[0].to(device)) #.to(device)\n",
    "                test_running_total += len(data[1])\n",
    "                test_running_right += (data[1].to(device) == torch.max(test_outputs, 1)[1]).sum() #to(device)\n",
    "\n",
    "            print(f'Test acc: {(test_running_right / test_running_total)*100:.4f}')\n",
    "\n",
    "        net.train()\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5HuGql2ZtMZ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40]. Step [1/100]. Loss: 0.0034. Acc: 53.0000. Test acc: 35.6400\n",
      "Epoch [2/40]. Step [1/100]. Loss: 0.0034. Acc: 56.0000. Test acc: 36.1200\n",
      "Epoch [3/40]. Step [1/100]. Loss: 0.0037. Acc: 51.0000. Test acc: 33.1200\n",
      "Epoch [4/40]. Step [1/100]. Loss: 0.0032. Acc: 54.2000. Test acc: 35.0900\n",
      "Epoch [5/40]. Step [1/100]. Loss: 0.0031. Acc: 56.6000. Test acc: 35.7400\n",
      "Epoch [6/40]. Step [1/100]. Loss: 0.0031. Acc: 56.8000. Test acc: 35.1300\n",
      "Epoch [7/40]. Step [1/100]. Loss: 0.0033. Acc: 53.8000. Test acc: 36.1800\n",
      "Epoch [8/40]. Step [1/100]. Loss: 0.0031. Acc: 54.6000. Test acc: 35.3200\n",
      "Epoch [9/40]. Step [1/100]. Loss: 0.0031. Acc: 57.2000. Test acc: 35.3500\n",
      "Epoch [10/40]. Step [1/100]. Loss: 0.0031. Acc: 56.0000. Test acc: 36.5500\n",
      "Epoch [11/40]. Step [1/100]. Loss: 0.0031. Acc: 58.2000. Test acc: 35.4200\n",
      "Epoch [12/40]. Step [1/100]. Loss: 0.0031. Acc: 58.8000. Test acc: 35.0500\n",
      "Epoch [13/40]. Step [1/100]. Loss: 0.0033. Acc: 56.8000. Test acc: 35.4500\n",
      "Epoch [14/40]. Step [1/100]. Loss: 0.0030. Acc: 59.6000. Test acc: 34.4900\n",
      "Epoch [15/40]. Step [1/100]. Loss: 0.0032. Acc: 57.6000. Test acc: 36.9900\n",
      "Epoch [16/40]. Step [1/100]. Loss: 0.0028. Acc: 62.0000. Test acc: 38.3900\n",
      "Epoch [17/40]. Step [1/100]. Loss: 0.0032. Acc: 56.0000. Test acc: 36.3000\n",
      "Epoch [18/40]. Step [1/100]. Loss: 0.0029. Acc: 58.4000. Test acc: 36.3500\n",
      "Epoch [19/40]. Step [1/100]. Loss: 0.0038. Acc: 47.8000. Test acc: 30.2000\n",
      "Epoch [20/40]. Step [1/100]. Loss: 0.0036. Acc: 51.0000. Test acc: 32.5700\n",
      "Epoch [21/40]. Step [1/100]. Loss: 0.0029. Acc: 57.2000. Test acc: 34.9700\n",
      "Epoch [22/40]. Step [1/100]. Loss: 0.0032. Acc: 57.8000. Test acc: 34.4400\n",
      "Epoch [23/40]. Step [1/100]. Loss: 0.0032. Acc: 56.0000. Test acc: 34.3200\n",
      "Epoch [24/40]. Step [1/100]. Loss: 0.0030. Acc: 58.6000. Test acc: 37.1800\n",
      "Epoch [25/40]. Step [1/100]. Loss: 0.0030. Acc: 58.4000. Test acc: 38.1900\n",
      "Epoch [26/40]. Step [1/100]. Loss: 0.0027. Acc: 63.2000. Test acc: 37.8900\n",
      "Epoch [27/40]. Step [1/100]. Loss: 0.0033. Acc: 61.2000. Test acc: 36.0800\n",
      "Epoch [28/40]. Step [1/100]. Loss: 0.0028. Acc: 63.8000. Test acc: 38.2800\n",
      "Epoch [29/40]. Step [1/100]. Loss: 0.0030. Acc: 62.2000. Test acc: 37.0400\n",
      "Epoch [30/40]. Step [1/100]. Loss: 0.0027. Acc: 63.2000. Test acc: 37.2000\n",
      "Epoch [31/40]. Step [1/100]. Loss: 0.0028. Acc: 60.2000. Test acc: 36.7300\n",
      "Epoch [32/40]. Step [1/100]. Loss: 0.0028. Acc: 59.2000. Test acc: 36.3600\n",
      "Epoch [33/40]. Step [1/100]. Loss: 0.0028. Acc: 61.2000. Test acc: 37.2200\n",
      "Epoch [34/40]. Step [1/100]. Loss: 0.0028. Acc: 60.8000. Test acc: 37.4800\n",
      "Epoch [35/40]. Step [1/100]. Loss: 0.0036. Acc: 53.4000. Test acc: 32.5000\n",
      "Epoch [36/40]. Step [1/100]. Loss: 0.0025. Acc: 63.8000. Test acc: 38.3500\n",
      "Epoch [37/40]. Step [1/100]. Loss: 0.0029. Acc: 63.6000. Test acc: 36.9600\n",
      "Epoch [38/40]. Step [1/100]. Loss: 0.0028. Acc: 61.4000. Test acc: 36.5200\n",
      "Epoch [39/40]. Step [1/100]. Loss: 0.0025. Acc: 64.6000. Test acc: 38.3200\n",
      "Epoch [40/40]. Step [1/100]. Loss: 0.0028. Acc: 60.6000. Test acc: 37.5500\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)  #to(device) .to(device)\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "        running_right += (labels == torch.max(outputs, 1)[1]).sum()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            net.eval()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {running_loss / running_items:.4f}. ' \\\n",
    "                  f'Acc: {(running_right / running_items)*100:.4f}', end='. ')\n",
    "            running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "\n",
    "            test_running_right, test_running_total = 0.0, 0.0\n",
    "            for i, data in enumerate(test_loader):\n",
    "\n",
    "                test_outputs = net(data[0].to(device)) #.to(device)\n",
    "                test_running_total += len(data[1])\n",
    "                test_running_right += (data[1].to(device) == torch.max(test_outputs, 1)[1]).sum() #to(device)\n",
    "\n",
    "            print(f'Test acc: {(test_running_right / test_running_total)*100:.4f}')\n",
    "\n",
    "        net.train()\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60]. Step [1/100]. Loss: 0.0026. Acc: 62.2000. Test acc: 38.0300\n",
      "Epoch [2/60]. Step [1/100]. Loss: 0.0023. Acc: 70.2000. Test acc: 37.2500\n",
      "Epoch [3/60]. Step [1/100]. Loss: 0.0025. Acc: 64.6000. Test acc: 35.7800\n",
      "Epoch [4/60]. Step [1/100]. Loss: 0.0026. Acc: 65.8000. Test acc: 34.1400\n",
      "Epoch [5/60]. Step [1/100]. Loss: 0.0028. Acc: 62.2000. Test acc: 37.5400\n",
      "Epoch [6/60]. Step [1/100]. Loss: 0.0027. Acc: 63.0000. Test acc: 37.3500\n",
      "Epoch [7/60]. Step [1/100]. Loss: 0.0029. Acc: 58.0000. Test acc: 33.8100\n",
      "Epoch [8/60]. Step [1/100]. Loss: 0.0024. Acc: 68.4000. Test acc: 36.8200\n",
      "Epoch [9/60]. Step [1/100]. Loss: 0.0026. Acc: 65.8000. Test acc: 36.9100\n",
      "Epoch [10/60]. Step [1/100]. Loss: 0.0026. Acc: 65.8000. Test acc: 37.7100\n",
      "Epoch [11/60]. Step [1/100]. Loss: 0.0025. Acc: 66.6000. Test acc: 37.3900\n",
      "Epoch [12/60]. Step [1/100]. Loss: 0.0021. Acc: 67.8000. Test acc: 39.5100\n",
      "Epoch [13/60]. Step [1/100]. Loss: 0.0024. Acc: 66.0000. Test acc: 38.7800\n",
      "Epoch [14/60]. Step [1/100]. Loss: 0.0023. Acc: 69.0000. Test acc: 36.8600\n",
      "Epoch [15/60]. Step [1/100]. Loss: 0.0025. Acc: 65.6000. Test acc: 35.8800\n",
      "Epoch [16/60]. Step [1/100]. Loss: 0.0022. Acc: 70.4000. Test acc: 37.5800\n",
      "Epoch [17/60]. Step [1/100]. Loss: 0.0022. Acc: 70.8000. Test acc: 39.5200\n",
      "Epoch [18/60]. Step [1/100]. Loss: 0.0022. Acc: 67.6000. Test acc: 38.2900\n",
      "Epoch [19/60]. Step [1/100]. Loss: 0.0018. Acc: 72.6000. Test acc: 40.1900\n",
      "Epoch [20/60]. Step [1/100]. Loss: 0.0024. Acc: 68.0000. Test acc: 37.6400\n",
      "Epoch [21/60]. Step [1/100]. Loss: 0.0022. Acc: 70.6000. Test acc: 38.2000\n",
      "Epoch [22/60]. Step [1/100]. Loss: 0.0024. Acc: 65.4000. Test acc: 38.9600\n",
      "Epoch [23/60]. Step [1/100]. Loss: 0.0019. Acc: 74.8000. Test acc: 39.2000\n",
      "Epoch [24/60]. Step [1/100]. Loss: 0.0022. Acc: 68.4000. Test acc: 39.2500\n",
      "Epoch [25/60]. Step [1/100]. Loss: 0.0022. Acc: 69.6000. Test acc: 38.1500\n",
      "Epoch [26/60]. Step [1/100]. Loss: 0.0029. Acc: 60.6000. Test acc: 36.7900\n",
      "Epoch [27/60]. Step [1/100]. Loss: 0.0019. Acc: 74.4000. Test acc: 38.5200\n",
      "Epoch [28/60]. Step [1/100]. Loss: 0.0024. Acc: 67.8000. Test acc: 38.1800\n",
      "Epoch [29/60]. Step [1/100]. Loss: 0.0022. Acc: 68.8000. Test acc: 35.6200\n",
      "Epoch [30/60]. Step [1/100]. Loss: 0.0023. Acc: 68.2000. Test acc: 38.5000\n",
      "Epoch [31/60]. Step [1/100]. Loss: 0.0020. Acc: 70.4000. Test acc: 38.0200\n",
      "Epoch [32/60]. Step [1/100]. Loss: 0.0022. Acc: 71.0000. Test acc: 38.3300\n",
      "Epoch [33/60]. Step [1/100]. Loss: 0.0019. Acc: 74.8000. Test acc: 37.4800\n",
      "Epoch [34/60]. Step [1/100]. Loss: 0.0022. Acc: 71.4000. Test acc: 39.2100\n",
      "Epoch [35/60]. Step [1/100]. Loss: 0.0030. Acc: 61.4000. Test acc: 33.4200\n",
      "Epoch [36/60]. Step [1/100]. Loss: 0.0021. Acc: 70.6000. Test acc: 39.5000\n",
      "Epoch [37/60]. Step [1/100]. Loss: 0.0021. Acc: 71.8000. Test acc: 39.9500\n",
      "Epoch [38/60]. Step [1/100]. Loss: 0.0019. Acc: 72.8000. Test acc: 38.8800\n",
      "Epoch [39/60]. Step [1/100]. Loss: 0.0019. Acc: 73.8000. Test acc: 38.5800\n",
      "Epoch [40/60]. Step [1/100]. Loss: 0.0020. Acc: 71.8000. Test acc: 38.8300\n",
      "Epoch [41/60]. Step [1/100]. Loss: 0.0016. Acc: 76.4000. Test acc: 37.8400\n",
      "Epoch [42/60]. Step [1/100]. Loss: 0.0018. Acc: 73.6000. Test acc: 39.4900\n",
      "Epoch [43/60]. Step [1/100]. Loss: 0.0022. Acc: 70.0000. Test acc: 36.1200\n",
      "Epoch [44/60]. Step [1/100]. Loss: 0.0020. Acc: 72.0000. Test acc: 38.4400\n",
      "Epoch [45/60]. Step [1/100]. Loss: 0.0018. Acc: 75.2000. Test acc: 38.4600\n",
      "Epoch [46/60]. Step [1/100]. Loss: 0.0023. Acc: 67.8000. Test acc: 38.2100\n",
      "Epoch [47/60]. Step [1/100]. Loss: 0.0024. Acc: 71.2000. Test acc: 35.8500\n",
      "Epoch [48/60]. Step [1/100]. Loss: 0.0021. Acc: 69.8000. Test acc: 39.2600\n",
      "Epoch [49/60]. Step [1/100]. Loss: 0.0021. Acc: 72.6000. Test acc: 37.4300\n",
      "Epoch [50/60]. Step [1/100]. Loss: 0.0021. Acc: 71.6000. Test acc: 38.6400\n",
      "Epoch [51/60]. Step [1/100]. Loss: 0.0018. Acc: 74.6000. Test acc: 38.6500\n",
      "Epoch [52/60]. Step [1/100]. Loss: 0.0016. Acc: 77.0000. Test acc: 38.3300\n",
      "Epoch [53/60]. Step [1/100]. Loss: 0.0019. Acc: 72.6000. Test acc: 39.4900\n",
      "Epoch [54/60]. Step [1/100]. Loss: 0.0020. Acc: 75.0000. Test acc: 37.6400\n",
      "Epoch [55/60]. Step [1/100]. Loss: 0.0019. Acc: 74.2000. Test acc: 38.5800\n",
      "Epoch [56/60]. Step [1/100]. Loss: 0.0018. Acc: 72.8000. Test acc: 38.7100\n",
      "Epoch [57/60]. Step [1/100]. Loss: 0.0018. Acc: 75.2000. Test acc: 38.2100\n",
      "Epoch [58/60]. Step [1/100]. Loss: 0.0017. Acc: 78.0000. Test acc: 38.3500\n",
      "Epoch [59/60]. Step [1/100]. Loss: 0.0025. Acc: 70.0000. Test acc: 33.3600\n",
      "Epoch [60/60]. Step [1/100]. Loss: 0.0016. Acc: 75.8000. Test acc: 38.2000\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)  #to(device) .to(device)\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "        running_right += (labels == torch.max(outputs, 1)[1]).sum()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            net.eval()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {running_loss / running_items:.4f}. ' \\\n",
    "                  f'Acc: {(running_right / running_items)*100:.4f}', end='. ')\n",
    "            running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "\n",
    "            test_running_right, test_running_total = 0.0, 0.0\n",
    "            for i, data in enumerate(test_loader):\n",
    "\n",
    "                test_outputs = net(data[0].to(device)) #.to(device)\n",
    "                test_running_total += len(data[1])\n",
    "                test_running_right += (data[1].to(device) == torch.max(test_outputs, 1)[1]).sum() #to(device)\n",
    "\n",
    "            print(f'Test acc: {(test_running_right / test_running_total)*100:.4f}')\n",
    "\n",
    "        net.train()\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5K8GTS3M402u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40]. Step [1/100]. Loss: 0.0017. Acc: 77.6000. Test acc: 38.7800\n",
      "Epoch [2/40]. Step [1/100]. Loss: 0.0019. Acc: 72.6000. Test acc: 37.1000\n",
      "Epoch [3/40]. Step [1/100]. Loss: 0.0017. Acc: 77.6000. Test acc: 38.5000\n",
      "Epoch [4/40]. Step [1/100]. Loss: 0.0015. Acc: 79.0000. Test acc: 38.0600\n",
      "Epoch [5/40]. Step [1/100]. Loss: 0.0017. Acc: 77.4000. Test acc: 39.1200\n",
      "Epoch [6/40]. Step [1/100]. Loss: 0.0017. Acc: 75.6000. Test acc: 39.0400\n",
      "Epoch [7/40]. Step [1/100]. Loss: 0.0015. Acc: 78.6000. Test acc: 38.9700\n",
      "Epoch [8/40]. Step [1/100]. Loss: 0.0016. Acc: 75.6000. Test acc: 37.1300\n",
      "Epoch [9/40]. Step [1/100]. Loss: 0.0017. Acc: 78.0000. Test acc: 38.5300\n",
      "Epoch [10/40]. Step [1/100]. Loss: 0.0018. Acc: 75.2000. Test acc: 38.8100\n",
      "Epoch [11/40]. Step [1/100]. Loss: 0.0021. Acc: 73.4000. Test acc: 38.1500\n",
      "Epoch [12/40]. Step [1/100]. Loss: 0.0016. Acc: 78.4000. Test acc: 39.5900\n",
      "Epoch [13/40]. Step [1/100]. Loss: 0.0018. Acc: 74.0000. Test acc: 38.3000\n",
      "Epoch [14/40]. Step [1/100]. Loss: 0.0021. Acc: 73.8000. Test acc: 35.2100\n",
      "Epoch [15/40]. Step [1/100]. Loss: 0.0017. Acc: 76.0000. Test acc: 38.5900\n",
      "Epoch [16/40]. Step [1/100]. Loss: 0.0018. Acc: 73.4000. Test acc: 39.2100\n",
      "Epoch [17/40]. Step [1/100]. Loss: 0.0014. Acc: 81.4000. Test acc: 39.9000\n",
      "Epoch [18/40]. Step [1/100]. Loss: 0.0020. Acc: 71.6000. Test acc: 38.2600\n",
      "Epoch [19/40]. Step [1/100]. Loss: 0.0018. Acc: 74.2000. Test acc: 38.7900\n",
      "Epoch [20/40]. Step [1/100]. Loss: 0.0017. Acc: 77.8000. Test acc: 38.5600\n",
      "Epoch [21/40]. Step [1/100]. Loss: 0.0017. Acc: 80.4000. Test acc: 37.6100\n",
      "Epoch [22/40]. Step [1/100]. Loss: 0.0018. Acc: 75.4000. Test acc: 36.5200\n",
      "Epoch [23/40]. Step [1/100]. Loss: 0.0014. Acc: 76.8000. Test acc: 38.9800\n",
      "Epoch [24/40]. Step [1/100]. Loss: 0.0021. Acc: 74.6000. Test acc: 37.3400\n",
      "Epoch [25/40]. Step [1/100]. Loss: 0.0017. Acc: 79.6000. Test acc: 38.7500\n",
      "Epoch [26/40]. Step [1/100]. Loss: 0.0012. Acc: 81.8000. Test acc: 39.2800\n",
      "Epoch [27/40]. Step [1/100]. Loss: 0.0017. Acc: 77.6000. Test acc: 39.4400\n",
      "Epoch [28/40]. Step [1/100]. Loss: 0.0015. Acc: 79.8000. Test acc: 39.2000\n",
      "Epoch [29/40]. Step [1/100]. Loss: 0.0021. Acc: 70.6000. Test acc: 37.8700\n",
      "Epoch [30/40]. Step [1/100]. Loss: 0.0014. Acc: 82.4000. Test acc: 38.1100\n",
      "Epoch [31/40]. Step [1/100]. Loss: 0.0014. Acc: 80.0000. Test acc: 39.1800\n",
      "Epoch [32/40]. Step [1/100]. Loss: 0.0020. Acc: 73.0000. Test acc: 36.8900\n",
      "Epoch [33/40]. Step [1/100]. Loss: 0.0015. Acc: 77.4000. Test acc: 38.7600\n",
      "Epoch [34/40]. Step [1/100]. Loss: 0.0017. Acc: 80.4000. Test acc: 38.4000\n",
      "Epoch [35/40]. Step [1/100]. Loss: 0.0012. Acc: 82.8000. Test acc: 40.0800\n",
      "Epoch [36/40]. Step [1/100]. Loss: 0.0013. Acc: 82.4000. Test acc: 39.1700\n",
      "Epoch [37/40]. Step [1/100]. Loss: 0.0015. Acc: 78.4000. Test acc: 40.2000\n",
      "Epoch [38/40]. Step [1/100]. Loss: 0.0016. Acc: 76.0000. Test acc: 37.8100\n",
      "Epoch [39/40]. Step [1/100]. Loss: 0.0018. Acc: 77.6000. Test acc: 37.1800\n",
      "Epoch [40/40]. Step [1/100]. Loss: 0.0017. Acc: 76.0000. Test acc: 37.4700\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "net.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)  #to(device) .to(device)\n",
    "\n",
    "        # обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "        running_right += (labels == torch.max(outputs, 1)[1]).sum()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            net.eval()\n",
    "\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {running_loss / running_items:.4f}. ' \\\n",
    "                  f'Acc: {(running_right / running_items)*100:.4f}', end='. ')\n",
    "            running_loss, running_items, running_right = 0.0, 0.0, 0.0\n",
    "\n",
    "            test_running_right, test_running_total = 0.0, 0.0\n",
    "            for i, data in enumerate(test_loader):\n",
    "\n",
    "                test_outputs = net(data[0].to(device)) #.to(device)\n",
    "                test_running_total += len(data[1])\n",
    "                test_running_right += (data[1].to(device) == torch.max(test_outputs, 1)[1]).sum() #to(device)\n",
    "\n",
    "            print(f'Test acc: {(test_running_right / test_running_total)*100:.4f}')\n",
    "\n",
    "        net.train()\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "HOjj8az6jF7D"
   },
   "outputs": [],
   "source": [
    "PATH_WEIGHTS = './cnn5_weights.pth'\n",
    "torch.save(net.state_dict(), PATH_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "kiO1TwJVjF94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model state_dict: \n",
      "bn1.weight \t torch.Size([3])\n",
      "bn1.bias \t torch.Size([3])\n",
      "bn1.running_mean \t torch.Size([3])\n",
      "bn1.running_var \t torch.Size([3])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "conv1.weight \t torch.Size([30, 3, 3, 3])\n",
      "conv1.bias \t torch.Size([30])\n",
      "bn2.weight \t torch.Size([30])\n",
      "bn2.bias \t torch.Size([30])\n",
      "bn2.running_mean \t torch.Size([30])\n",
      "bn2.running_var \t torch.Size([30])\n",
      "bn2.num_batches_tracked \t torch.Size([])\n",
      "conv2.weight \t torch.Size([64, 30, 3, 3])\n",
      "conv2.bias \t torch.Size([64])\n",
      "bn3.weight \t torch.Size([64])\n",
      "bn3.bias \t torch.Size([64])\n",
      "bn3.running_mean \t torch.Size([64])\n",
      "bn3.running_var \t torch.Size([64])\n",
      "bn3.num_batches_tracked \t torch.Size([])\n",
      "conv3.weight \t torch.Size([128, 64, 3, 3])\n",
      "conv3.bias \t torch.Size([128])\n",
      "bn4.weight \t torch.Size([128])\n",
      "bn4.bias \t torch.Size([128])\n",
      "bn4.running_mean \t torch.Size([128])\n",
      "bn4.running_var \t torch.Size([128])\n",
      "bn4.num_batches_tracked \t torch.Size([])\n",
      "conv4.weight \t torch.Size([128, 128, 3, 3])\n",
      "conv4.bias \t torch.Size([128])\n",
      "bn5.weight \t torch.Size([128])\n",
      "bn5.bias \t torch.Size([128])\n",
      "bn5.running_mean \t torch.Size([128])\n",
      "bn5.running_var \t torch.Size([128])\n",
      "bn5.num_batches_tracked \t torch.Size([])\n",
      "conv5.weight \t torch.Size([256, 128, 3, 3])\n",
      "conv5.bias \t torch.Size([256])\n",
      "bn6.weight \t torch.Size([256])\n",
      "bn6.bias \t torch.Size([256])\n",
      "bn6.running_mean \t torch.Size([256])\n",
      "bn6.running_var \t torch.Size([256])\n",
      "bn6.num_batches_tracked \t torch.Size([])\n",
      "conv6.weight \t torch.Size([256, 256, 3, 3])\n",
      "conv6.bias \t torch.Size([256])\n",
      "bn7.weight \t torch.Size([256])\n",
      "bn7.bias \t torch.Size([256])\n",
      "bn7.running_mean \t torch.Size([256])\n",
      "bn7.running_var \t torch.Size([256])\n",
      "bn7.num_batches_tracked \t torch.Size([])\n",
      "conv7.weight \t torch.Size([512, 256, 3, 3])\n",
      "conv7.bias \t torch.Size([512])\n",
      "bn8.weight \t torch.Size([512])\n",
      "bn8.bias \t torch.Size([512])\n",
      "bn8.running_mean \t torch.Size([512])\n",
      "bn8.running_var \t torch.Size([512])\n",
      "bn8.num_batches_tracked \t torch.Size([])\n",
      "fc1.weight \t torch.Size([1024, 2048])\n",
      "fc1.bias \t torch.Size([1024])\n",
      "fc2.weight \t torch.Size([512, 1024])\n",
      "fc2.bias \t torch.Size([512])\n",
      "out.weight \t torch.Size([100, 512])\n",
      "out.bias \t torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model state_dict: \")\n",
    "for param in net.state_dict():\n",
    "    print(param, \"\\t\", net.state_dict()[param].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_CCN_5(\n",
       "  (dp1): Dropout(p=0.2, inplace=False)\n",
       "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv2d(3, 30, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn2): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(30, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dp2): Dropout(p=0.3, inplace=False)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dp3): Dropout(p=0.3, inplace=False)\n",
       "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dp4): Dropout(p=0.2, inplace=False)\n",
       "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dp5): Dropout(p=0.2, inplace=False)\n",
       "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (dp_fc1): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (dp_fc2): Dropout(p=0.3, inplace=False)\n",
       "  (out): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net_CCN_5().to(device)\n",
    "PATH_WEIGHTS = './cnn5_weights.pth'\n",
    "net.load_state_dict(torch.load(PATH_WEIGHTS))\n",
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final Test Accuracy of the model: 38.080000000000005 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('The final Test Accuracy of the model: {} %'.format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Созданая сеть неплохо с падингам с модом reflection на начальных этапах довольно неплохо обучается\n",
    "\n",
    "* Однако на 40 эпохе начинается переобучение. Для его устранения дополнительно необходимо подобрать скорость обучения и % Dropout в самой сети так же можно добавить l2 регуляризацию в ходе обучения"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
